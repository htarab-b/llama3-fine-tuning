# -*- coding: utf-8 -*-
"""Fine-Tuning Llama3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Nu3XIpe3of2GOIqUgX5ZaiFEolvfThy
"""

# !pip install unsloth
# !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
# !pip install -U transformers accelerate datasets trl bitsandbytes

from unsloth import FastLanguageModel

# Configuration settings
max_seq_length = 2048
load_in_4bit = True

# Load the Llama-3 model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct",  # Ensure this model exists
    max_seq_length=max_seq_length,
    load_in_4bit=load_in_4bit,
)

# Apply LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

from datasets import load_dataset
from unsloth import get_chat_template

# Load dataset
dataset = load_dataset("json", data_files="./QA Generator/train.jsonl")

# Apply chat template correctly
tokenizer = get_chat_template(tokenizer, chat_template="llama-3")

def format_text(example):
    instruction = example["instruction"]
    input_text = example["input"]
    output_text = example["output"]

    messages = [
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user", "content": f"{instruction}\n{input_text}"},
        {"role": "assistant", "content": output_text},
    ]

    formatted_text = tokenizer.apply_chat_template(messages, tokenize=False)
    return {"text": formatted_text}

# Apply formatting
dataset = dataset.map(format_text)

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

# Initialize the trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    dataset_num_proc=2,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=30,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        output_dir="outputs",
        report_to="none",
    ),
)

# Mask user inputs to train only on assistant responses
from unsloth.chat_templates import train_on_responses_only

trainer = train_on_responses_only(
    trainer,
    instruction_part="user",
    response_part="assistant",
)

# Train the model
trainer.train()

model.save_pretrained("llama3-finetuned")
tokenizer.save_pretrained("llama3-finetuned")
